# Emotion-Prediction-using-DCNN

# Introduction:

Emotion is a cumulative expression of the consciousness and actions of humans. This essentially mirrors the experience and behavior of humans. In our daily lives emotion often plays a critical role, particularly in human-human interaction. In addition, emotional perception based on the computer system has also become a central part of the advanced brain-machine interaction systems. Therefore, emotion recognition has become a common subject in modern neuroscience, psychology, neural engineering, and computer science as well as its great significance and wide use.
Recognition of emotions can be split into two categories: one based on non-physiological signals and the other based on physiological signals. Some earlier emotion recognition experiments are focused on non-physiological cues, such as the identification of facial expression and voice-dependent emotion. Face expressions and tone of voice can also be intentionally obscured so that the method based on them is clearly not dependable. At the other side, the approach focused at physiological signals, referring to electroencephalography (EEG), electromyogram (EMG), electrocardiogram (ECG), skin resistance (SC), pulse rate and breathing signals, tend to be more efficient and accurate since people are unable to regulate them deliberately. In this project, we have aimed to predict to analyze how different portions of the human brain respond to different stimuli.

# Data Description:
EEG dataset has been used for Emotion Recognition. 15 Chinese video clips (positive, neutral and negative emotions) were chosen from the collection of materials as stimuli used in the experiments. The selection criteria for movie clips are as follows: 
(a) the length of the whole experiment should not be too long in case it will make subjects fatigue. 
(b) the videos should be understood without explanation; and 
(c) the videos should elicit a single desired target emotion. 
The length of each video clip is about 4 minutes. Each video clip is well edited to create coherent emotion stimulating and maximize emotional meanings. There are totally 15 trials for each experiment. There is a 15s hint before each clips and 10s feedback after each clip. The order of presentation is arranged so that two movie clips targeting the same emotion are not shown consecutively. For the feedback, participants are told to report their emotional reactions to each film clip by completing the questionnaire immediately after watching each clip. A new effective EEG feature named differential entropy is proposed to represent the characteristics associated with emotional states.
In the "data" folder, there are train and test dataset containing down sampled with sampling frequency 200Hz in order to speed up the computation, preprocessed and segmented versions of the EEG differential entropy data.
The dataset containing extracted differential entropy (DE) features of the EEG signals. These data is well-suited to those who want to quickly test a classification method without processing the raw EEG data. The training set contains a total of 84420 data and testing set contains 58128 data. Each piece of data contains 310 values representing the data of 62 electrodes on 5 frequencies. These 310 values can be considered as the feature/attribute values for training a model.

# Methods Used:
  ## Data Augmentation:
The term data augmentation refers to methods for constructing iterative optimization or sampling algorithms via the introduction of unobserved data or latent variables. This can reduce overfitting of models that are fed with small datasets, because these do not generalize well from the validation and test set. Data augmentation means increasing the number of data points. In terms of images, it may mean increasing the number of images in the dataset. In terms of traditional row/column format data, it means increasing the number of rows or objects. Data augmentation is a method that artificially creates new data based on the modifications of the existing data. The heuristics underlying these modifications are very dependent on which processes are suitable for the classification task at issue.

   ### 1.	Data augmentation using Autoencoder

An autoencoder network is a pair of two connected networks, an encoder and a decoder. An encoder network takes in an input, and converts it into a smaller, dense representation, which the decoder network can use to convert it back to the original input. An encoder is a network that takes in an input and produces a much smaller representation (the encoding), that contains enough information for the next part of the network to process it into the desired output format. Typically, the encoder is trained together with the other parts of the network, optimized via back-propagation, to produce encodings specifically useful for the task at hand. In CNNs, the encodings produced are such that they are specifically useful for classification. Autoencoders take this idea, and slightly flip it on its head, by making the encoder generate encodings specifically useful for reconstructing its own input. 
The entire network is usually trained. The loss function is usually either the mean-squared error or cross-entropy between the output and the input, known as the reconstruction loss, which penalizes the network for creating outputs different from the input. As the encoding (which is simply the output of the hidden layer in the middle) has far less units than the input, the encoder must choose to discard information. The encoder learns to preserve as much of the relevant information as possible in the limited encoding, and intelligently discard irrelevant parts. The decoder learns to take the encoding and properly reconstruct it into a full image.

   ### 2.	Data augmentation using Standard Scaling

The image can be scaled outward or inward. While scaling outward, the final image size will be larger than the original image size. Most image frameworks cut out a section from the new image, with size equal to the original image. We’ll deal with scaling inward in the next section, as it reduces the image size, forcing us to make assumptions about what lies beyond the boundary.  

  ## Feature Extraction:
Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. A characteristic of these large data sets is many variables that require a lot of computing resources to process. Feature extraction is the name for methods that select and /or combine variables into features, effectively reducing the amount of data that must be processed, while still accurately and completely describing the original data set.

The process of feature extraction is useful when you need to reduce the number of resources needed for processing without losing important or relevant information. Feature extraction can also reduce the amount of redundant data for a given analysis. Also, the reduction of the data and the machine’s efforts in building variable combinations (features) facilitate the speed of learning and generalization steps in the machine learning process.

   ### 1.	Feature extraction using Linear Discriminant Analysis (LDA)

Linear discriminant analysis (LDA) is a traditional statistical technique that reduces dimensionality while preserving as much of the class discriminatory information as possible. The conventional form of the LDA assumes that all the data are available in advance and the LDA feature space is computed by finding the eigen decomposition of an appropriate matrix. However, there are situations where the data are presented in a sequence and the LDA features are required to be updated incrementally by observing the new incoming samples.

   ### 2.	Feature extraction using Autoencoder

Feature extraction becomes increasingly important as data grows high dimensional. Autoencoder as a neural network based feature extraction method achieves great success in generating abstract features of high dimensional data. However, it fails to consider the relationships of data samples which may affect experimental results of using original and new features.

# Proposed Model:
 
EEG dataset is already split into train and test folders when you download the dataset. But for applying the data augmentation technique on the given data, we have merged the train and test folders and then used standard scaling as our final data augmentation technique. This standard scaling has been used by centering the data before scaling, and it scaled the data to unit variance i.e. unit standard deviation. 
After scaling the dataset, we performed the feature extraction using LDA method to reduce the dimensionality of the input by projecting it to the most discriminative directions. The number of components used for dimensionality reduction used is 2. We have split the dataset according to the requirement of the project. The training set contains a total of 84420 data and testing set contains 58128 data. 
After extracting the features through dimension reduction, we forwarded it through the sequential convolutional model that we prepared.
 
This model consists of 3 Convolutional 1D layers, 5 Dense layers with ReLu activation function. At the end one Dense Layer with Softmax activation layer. We passed the input to the convolutional 1D layer with kernel size 2x2. Then forwarded it to dense layer of hidden nodes 32. Then, applied the 2 convolutional with kernel 2x1 size. And forwarded it to the further 5 dense layers with neurons 64, 32, 16, 8 and 3 (number of outputs). Last layer is having the softmax activation function. We trained the model with ‘categorical_crossentropy’ loss function and ‘adam’ optimizer. We tried so many combinations for getting the highest accuracy of around 88.0464%. We will discuss that in detail in discussions section and python code has attached in below section.
